{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install fasttext\n",
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "E818jhLlauXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TwGj0TSWO4x"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AI4Bharat/IndicLID.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/IndicLID/Inference\""
      ],
      "metadata": {
        "id": "LTeZjvTBanun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir models\n",
        "%cd \"/content/IndicLID/Inference/models\""
      ],
      "metadata": {
        "id": "qnEexoohl8FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-bert.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftn.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftr.zip"
      ],
      "metadata": {
        "id": "YuPIMRGIcJTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip indiclid-bert.zip\n",
        "!unzip indiclid-ftn.zip\n",
        "!unzip indiclid-ftr.zip"
      ],
      "metadata": {
        "id": "JAXjxBEKmCmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd \"/content/IndicLID/\"\n",
        "%cd \"/content/IndicLID/Inference\""
      ],
      "metadata": {
        "id": "MJEnTEkYcgtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ai4bharat.IndicLID import IndicLID\n",
        "\n",
        "IndicLID_model = IndicLID(input_threshold = 0.5, roman_lid_threshold = 0.6)"
      ],
      "metadata": {
        "id": "caLffPG_MciT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Function to extract all 'body' fields from comments and their replies\n",
        "def extract_bodies(data):\n",
        "    bodies = []\n",
        "    for item in data:  # Assuming the top-level structure is a list\n",
        "        if 'comments' in item:  # Check if the 'comments' key exists\n",
        "            for comment in item['comments']:\n",
        "                if 'body' in comment:  # Check if the 'body' key exists in the comment\n",
        "                    clean_body = comment['body'].replace('\\n', ' ')  # Replace newline characters with a space\n",
        "                    bodies.append(clean_body)\n",
        "                if 'reply' in comment:  # Check if there are replies\n",
        "                    for reply in comment['reply']:\n",
        "                        if 'body' in reply:  # Check if the 'body' key exists in the reply\n",
        "                            clean_reply_body = reply['body'].replace('\\n', ' ')  # Similarly, clean up the reply body\n",
        "                            bodies.append(clean_reply_body)\n",
        "    return bodies\n",
        "\n",
        "# Function to read JSON data from a file and extract bodies\n",
        "def read_and_extract(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:  # Open the file in read mode\n",
        "        data = json.load(file)  # Load JSON data from the file\n",
        "        return extract_bodies(data)  # Extract bodies using the function defined earlier\n",
        "\n",
        "# Specify the path to your JSON file\n",
        "file_path = '/content/MergedDelhi.json'  # Update this to the path of your JSON file\n",
        "comment_bodies = read_and_extract(file_path)\n",
        "print(comment_bodies)\n",
        "test_samples1 = comment_bodies\n",
        "seen = set()\n",
        "unique_list = []\n",
        "for item in test_samples1:\n",
        "    if item not in seen:\n",
        "        unique_list.append(item)\n",
        "        seen.add(item)\n"
      ],
      "metadata": {
        "id": "Fpd_C71swhCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = ['hi i am rebekah, i go to school in India, I am 22 years old', 'mera nam rebekah hai, apka kya hai', 'hi bro'\n",
        "]"
      ],
      "metadata": {
        "id": "1Pk4y9SGM0S2"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "outputs = IndicLID_model.batch_predict(unique_list, batch_size)"
      ],
      "metadata": {
        "id": "tzGcNS1SX9da"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "r-eWigM3l8ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code to get a list of only detected english comments"
      ],
      "metadata": {
        "id": "2AS1UvAKmx8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_list = []\n",
        "count=0\n",
        "for item in outputs:\n",
        "    if item[1] == 'eng_Latn':\n",
        "        #print(item[0])\n",
        "        english_list.append(item)\n",
        "        count = count + 1\n",
        "print(count)"
      ],
      "metadata": {
        "id": "Rs68gQv_23uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stopwords and emojis"
      ],
      "metadata": {
        "id": "O7FNbJCRm_gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # Unicode ranges to match typical emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # Replacing emojis with empty string\n",
        "\n",
        "# Initialize the English stopwords set\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Process the data using indices\n",
        "cleaned_data = []\n",
        "for item in english_list:\n",
        "    if item[1] == 'eng_Latn':\n",
        "        text = remove_emojis(item[0])\n",
        "        tokens = word_tokenize(text)\n",
        "        filtered_text = ' '.join(word for word in tokens if word.lower() not in stop_words)\n",
        "        cleaned_data.append((filtered_text, item[1]))\n",
        "\n"
      ],
      "metadata": {
        "id": "s8BWWGQybfd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plot of frequency of predicted languages"
      ],
      "metadata": {
        "id": "ZMSXXDZ2nJId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Extracting the language and script codes\n",
        "languages = [item[1] for item in outputs]\n",
        "\n",
        "# Counting the frequency of each language\n",
        "language_counts = Counter(languages)\n",
        "\n",
        "# Creating labels and values for the plot\n",
        "labels = list(language_counts.keys())\n",
        "values = list(language_counts.values())\n",
        "\n",
        "# Creating the bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(labels, values, color='yellow')\n",
        "plt.xlabel('Language')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Predicted Languages')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W5BfLYmWAWZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks for any duplicates in data"
      ],
      "metadata": {
        "id": "q76sKITenzTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "temp=0\n",
        "\n",
        "# Extracting only the text snippets\n",
        "texts = [item[0] for item in outputs]\n",
        "\n",
        "# Counting the frequency of each text snippet\n",
        "text_counts = Counter(texts)\n",
        "\n",
        "# Printing the text snippets that appear more than once\n",
        "for text, count in text_counts.items():\n",
        "    if count > 1:\n",
        "        temp = temp + 1\n",
        "print(temp)\n"
      ],
      "metadata": {
        "id": "TJ0E5qdlI4Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Cloud of all the identified english text"
      ],
      "metadata": {
        "id": "DALMNlUCn4yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Combine all text snippets into one large string\n",
        "text = \" \".join([item[0] for item in cleaned_data])\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(text)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ldNmF_T-OaIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table with count and percentages"
      ],
      "metadata": {
        "id": "qLaH2cgIoDs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Extracting the language codes\n",
        "languages = [item[1] for item in outputs]\n",
        "\n",
        "# Counting occurrences of each language\n",
        "language_counts = Counter(languages)\n",
        "\n",
        "# Calculating total number of entries for percentage calculation\n",
        "total = sum(language_counts.values())\n",
        "\n",
        "# Creating a sorted list of tuples from the language_counts dictionary\n",
        "sorted_languages = sorted(language_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Calculating percentage and printing results\n",
        "print(\"Language\\tCount\\tPercentage\")\n",
        "for language, count in sorted_languages:\n",
        "    percentage = (count / total) * 100\n",
        "    print(f\"{language}\\t{count}\\t{percentage:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ISUo4unMRZW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}