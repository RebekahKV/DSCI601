# -*- coding: utf-8 -*-
"""LDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cjY8lcov5BzFEnkAYDlDmZYBmFIcx7JO

This script performs topic modeling the Latent Dirichlet Allocation (LDA) algorithm. 
"""

!pip install gensim pandas nltk

import pandas as pd
import gensim
from gensim import corpora
from gensim.models import LdaModel

cleaned_data = pd.read_csv('/content/Comments5000.csv')

tokenized_data = cleaned_data['Comment'].apply(lambda x: x.split()).tolist()  # tokenizing

# Prepare data for topic modeling
dictionary = corpora.Dictionary(tokenized_data)
corpus = [dictionary.doc2bow(text) for text in tokenized_data]

# Training the LDA model
num_topics = 5
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=25)

# Assign each comment to its dominant topic
dominant_topics = []

for doc in corpus:
    topic_distribution = lda_model.get_document_topics(doc)
    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]  # Get the index of the highest probability topic
    dominant_topics.append(dominant_topic)

# Count comments per topic
topic_counts = pd.Series(dominant_topics).value_counts()

# Calculate percentages
total_comments = len(dominant_topics)
topic_percentages = (topic_counts / total_comments) * 100

# Prepare the output DataFrame with topics, tokens, and percentages
output_rows = []

for idx in range(num_topics):
    # Get most relevant tokens for the topic
    topic_info = lda_model.get_topic_terms(idx, topn=6)
    tokens = ", ".join([dictionary[id] for id, _ in topic_info])
    overall_percentage = topic_percentages.get(idx, 0)

    output_rows.append({
        'Topic': f'Topic {idx + 1}',
        'Most Relevant Tokens': tokens,
        'Overall Percentage': f"{overall_percentage:.2f}%"
    })


output_df = pd.DataFrame(output_rows)
output_df.head(5)

# Visualization of comments per topic
import matplotlib.pyplot as plt
topic_counts = topic_counts.reindex(range(num_topics), fill_value=0)
plt.figure(figsize=(10, 6))
topic_counts.plot(kind='bar', color='skyblue')

plt.xlabel('Topics')
plt.ylabel('Number of Comments')
plt.title('Number of Comments per Topic')
plt.xticks(rotation=0)
plt.show()

from gensim import corpora
from gensim.models import LdaModel, CoherenceModel

tokenized_data = cleaned_data['Comment'].apply(lambda x: x.split()).tolist()
dictionary = corpora.Dictionary(tokenized_data)
corpus = [dictionary.doc2bow(text) for text in tokenized_data]

# Function to compute coherence values for numbers of topics (5, 10, 15)
def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5):
    model_list = []
    coherence_values = []

    for num_topics in range(start, limit + 1, step):
        model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, iterations=400, alpha='auto')
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())
    return model_list, coherence_values

# Compute coherence values for topics in the range of 5 to 15
model_list, coherence_values = compute_coherence_values(dictionary, corpus, tokenized_data, limit=15)

# Create a DataFrame to store the results
results_df = pd.DataFrame({
    'Number of Topics': range(5, 16, 5),
    'Coherence Score': coherence_values
})

print(results_df)

# Plot the coherence scores
plt.figure(figsize=(10, 5))
plt.plot(results_df['Number of Topics'], results_df['Coherence Score'], marker='o')
plt.title('Coherence Scores')
plt.xlabel('Number of Topics')
plt.ylabel('Coherence Score')
plt.xticks(results_df['Number of Topics'])
plt.show()

# Training LDA with number of topics = 10
cleaned_data = pd.read_csv('/content/Comments5000.csv')
tokenized_data = cleaned_data['Comment'].apply(lambda x: x.split()).tolist()
dictionary = corpora.Dictionary(tokenized_data)
corpus = [dictionary.doc2bow(text) for text in tokenized_data]

# Trainingthe LDA model
optimal_topics = 10
final_model = LdaModel(
    corpus,
    num_topics = optimal_topics,
    id2word = dictionary,
    passes = 15,
    iterations = 400,
    alpha = 'auto'
)

# Assign each comment to its dominant topic
dominant_topics = []
for doc in corpus:
    topic_distribution = final_model.get_document_topics(doc)
    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]
    dominant_topics.append(dominant_topic)

# Count comments per topic
topic_counts = pd.Series(dominant_topics).value_counts()
total_comments = len(dominant_topics)
topic_percentages = (topic_counts / total_comments) * 100

# Prepare the output DataFrame with topics and tokens
output_rows = []
for idx in range(optimal_num_topics):
    topic_info = final_model.get_topic_terms(idx, topn=6)
    tokens = ", ".join([dictionary[id] for id, _ in topic_info])
    overall_percentage = topic_percentages.get(idx, 0)
    output_rows.append({
        'Topic': f'Topic {idx + 1}',
        'Most Relevant Tokens': tokens,
        'Overall Percentage': f"{overall_percentage:.2f}%"
    })

# Create and display output
final_output = pd.DataFrame(output_rows)
final_output.head(20)
